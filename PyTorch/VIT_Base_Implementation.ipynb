{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zRVv1AWvPGAI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VIT-Base Implementation"
      ],
      "metadata": {
        "id": "G6EcNfnuPCRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install a Library and Import Packages"
      ],
      "metadata": {
        "id": "zRVv1AWvPGAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiYk96i-7UYS",
        "outputId": "17fa79a2-5f46-4a60-d002-87f5aa00072c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "aY-2xtrt81Pw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "lrPPrPG5PLgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embedded Patches**\n",
        "\n",
        "**Getting x_p**\n",
        "\n",
        "First, divide images into multiple patches. Let's say the original image tensor has shape of `(batch_size, num_channels, p_height * h_count, p_width * w_count)`, here we're dividing the images with p_height * p_width size of patches. So for each single image we would have h_count * w_count number of patches, where each patch has shape of (p_height, p_width, num_channels). Here we flatten this into p_height * p_width * num_channels. So as a result x_p would have shape of (batch_size, h_count * w_count, h_count*w_count*num_channels).\n",
        "\n",
        "\n",
        "\n",
        "**Linear Projection into D dimension**\n",
        "\n",
        "- Doing things inside [...]\n",
        "\n",
        "We project this with the Linear Projection 'E',  nn.Linear(h_count * w_count * num_channels, d_model). As a result x_p has shape of (batch_size, N = h_count * w_count, d_model). (The paper denotes h_count * w_count as N.). Plus, the x_class has shape of (batch_size, 1, d_model). So the thing inside [...] in Eq(1) have shape of `(batch_size, N+1, d_model)`\n",
        "\n",
        "\n",
        "- [...] + E_pos\n",
        "E_pos have shape of `(batch_size, N+1, d_model)` too. So [...] + E_pos has also have shape of `(batch_size, N+1, d_model)`. As a result after the Eq(1), the shape is (batch_size, N+1, d_model).\n",
        "\n",
        "**Enocder**\n",
        "\n",
        "Now we pass the z_0 to encoder. Here MSA means MultiHeadSelfAttention and LN means Layer Normalization.\n",
        "\n",
        "\n",
        "**Classification Head**\n",
        "\n",
        "\n",
        "Although in the diagram in the paper, it seems like the output of the encoder is being directly passed to the classicifcation head, but if u take a look at the their code, it first go through the LN, then got passed to classification head.\n",
        "\n",
        "\n",
        "Use different classification head depending on whether ur pre-training or fine-tuning:\n",
        "\n",
        "**Pre-training**: MLP with one hidden layer\n",
        "\n",
        "**Fine-Tuning**: a single linear layer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZeiQaJTQHDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 768\n",
        "mlp_size = 3072\n",
        "heads=12\n",
        "layers=12\n",
        "N = 256\n",
        "batch_size = 2"
      ],
      "metadata": {
        "id": "PMRHphzouTj4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, ln_input_shape, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(ln_input_shape)\n",
        "    self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=0.2)\n",
        "    self.layer_norm12 = nn.LayerNorm(ln_input_shape)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, mlp_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_size, d_model),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    input shape:  # (batch_sizes, N+1, d_model)\n",
        "    output shape:  # (batch_sizes, N+1, d_model)\n",
        "    \"\"\"\n",
        "    x_prime = self.layer_norm1(x) # (batch_sizes, N+1, d_model)\n",
        "\n",
        "    first_out, _ = self.mha(x_prime, x_prime, x_prime)  # (batch_sizes, N+1, d_model)\n",
        "    first_out = first_out + x  # (batch_sizes, N+1, d_model)\n",
        "\n",
        "    x_prime = self.layer_norm2(first_out)  # (batch_sizes, N+1, d_model)\n",
        "    second_out = self.mlp(x_prime)  # (batch_sizes, N+1, d_model)\n",
        "    return second_out + first_out  # (batch_sizes, N+1, d_model)"
      ],
      "metadata": {
        "id": "lU5Ww5V9wvut"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, p1, p2, d_model, batch_size, H, W, ln_input_shape, heads):\n",
        "    super().__init__()\n",
        "    self.p1 = p1\n",
        "    self.p2 = p2\n",
        "    self.E_projection = nn.Linear(588, d_model)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(ln_input_shape=ln_input_shape, d_model=d_model, num_heads=heads)\n",
        "            for _ in range(12)\n",
        "        ])\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, N + 1, d_model))\n",
        "\n",
        "\n",
        "  def forward(self, img):\n",
        "    \"\"\"\n",
        "    input shape: (batch_sizes, num_channels, height, width)\n",
        "    output shape: (batch_sizes, N+1, D)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = img.size[0]\n",
        "    img_patches = rearrange(img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)', patch_x=self.p1, patch_y=self.p2) #(batch_siez, N, p_height*p_width*c)\n",
        "\n",
        "    # now Apply E matrix\n",
        "    img_patches = self.E_projection(img_patches) # (batch_sizes, N, D)\n",
        "\n",
        "    # add CLS token which has shape of (batch_sizes, 1, D)\n",
        "    cls_token = self.cls_token.expand(batch_size, -1, -1) # (batch_sizes, 1, D)\n",
        "    img_patches = torch.cat((cls_token, img_patches), dim=1) # (batch_sizes, N+1, D)\n",
        "\n",
        "    img_patches = img_patches + img_patches\n",
        "    # repeat encoder layer 12 times\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      img_patches = encoder_layer(img_patches)\n",
        "\n",
        "    # (batch_sizes, N+1, D)\n",
        "\n",
        "    return img_patches"
      ],
      "metadata": {
        "id": "EhpZdsVT2Kkd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2"
      ],
      "metadata": {
        "id": "PXh6x6-C9W9B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(p1=14, p2=14, d_model=768, batch_size=batch_size, H=224, W=224, ln_input_shape=(N + 1, d_model), heads=12)"
      ],
      "metadata": {
        "id": "vZcPL4YajIy0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = model(torch.randn(batch_size, 3, 224, 224))"
      ],
      "metadata": {
        "id": "_JOVunYEjb0X"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output.shape"
      ],
      "metadata": {
        "id": "ElJu7vErj9IK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters: {num_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Im7tlASycBQ",
        "outputId": "7afe7b8f-bc6a-464f-c6cd-bf6ec7e261e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 95142144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1sadCC3PhA1n"
      }
    }
  ]
}