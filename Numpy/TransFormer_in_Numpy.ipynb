{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "L-Gy9jO17Dee"
      },
      "outputs": [],
      "source": [
        "!pip install -q colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "oZJGgx3DOS16"
      },
      "outputs": [],
      "source": [
        "from myPyTorch import *\n",
        "import numpy as np\n",
        "from colorama import Fore, Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "zTHCS-EbP1SQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "source_length = 10\n",
        "target_length = 10\n",
        "d_model = 256\n",
        "forward_dim = 512\n",
        "vocab_size = 5000\n",
        "num_layers=4\n",
        "num_heads=8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKRXbo4lyGpM"
      },
      "source": [
        "### FeedForward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "iFI9O7dpyIiv"
      },
      "outputs": [],
      "source": [
        "class FeedForward:\n",
        "  def __init__(self, d_model, forward_dim):\n",
        "    self.layer1 = Linear(d_model, forward_dim)\n",
        "    self.layer2 = Linear(forward_dim, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1.forward(x)\n",
        "    x = self.layer2.forward(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzWbeae3PmW5",
        "outputId": "1847fbeb-9870-4c3f-c632-9431c6b7cf64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 256)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.rand(batch_size, d_model)\n",
        "ff = FeedForward(d_model, forward_dim)\n",
        "output = ff.forward(x)\n",
        "print(output.shape) # (batch_size, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjtnjoHe6dDt"
      },
      "source": [
        "### LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "19dbFy6lQZca"
      },
      "outputs": [],
      "source": [
        "class LayerNorm:\n",
        "  def __init__(self, d_model, epsilon=1e-5):\n",
        "    self.gamma = np.ones(d_model)\n",
        "    self.beta = np.zeros(d_model)\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / np.sqrt(variance + self.epsilon)\n",
        "    output = self.gamma * normalized_x + self.beta\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQXtAya0zHP4"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "namyNrhZQ2kb"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer:\n",
        "  def __init__(self, d_model, num_heads, source_length, target_length):\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads, source_length, target_length)\n",
        "    self.ff = FeedForward(d_model, forward_dim)\n",
        "    self.layer_norm = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    input: (batch_size, source_length, d_model)\n",
        "    output: (batch_size, source_length, d_model)\n",
        "    '''\n",
        "    x_prime = x\n",
        "    x = self.mha.forward(x, x, x)  # (N, L, d_model)\n",
        "    x = self.layer_norm.forward(x + x_prime)\n",
        "\n",
        "    x_prime = x\n",
        "    x = self.ff.forward(x) # (N, L, d_model)\n",
        "    x = self.layer_norm.forward(x + x_prime)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4aRfhS0zas-",
        "outputId": "cf826245-1315-4d94-83f5-cefc476203b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 10, 256)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/myPyTorch.py:313: RuntimeWarning: overflow encountered in exp\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
            "/content/myPyTorch.py:313: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n"
          ]
        }
      ],
      "source": [
        "# test encoder layer\n",
        "x = np.random.randn(batch_size, source_length, d_model)\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, source_length, target_length)\n",
        "output = encoder_layer.forward(x)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-vM2GAH22Gx",
        "outputId": "c6055b70-f550-43cc-8d92-dd2b9d6aa65f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 10, 256)"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.random.randint(0, vocab_size, (batch_size, source_length)).astype(np.int64)\n",
        "layer = Embedding(vocab_size, d_model)\n",
        "layer.forward(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "etqDP-WaRAf2"
      },
      "outputs": [],
      "source": [
        "class Encoder:\n",
        "  def __init__(self, seq_length, d_model, num_heads, forward_dim, num_layers, vocab_size):\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding_layer = Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = self.create_positional_encoding(seq_length, d_model)\n",
        "    self.encoder_layers = [EncoderLayer(d_model, num_heads, source_length, target_length) for _ in range(num_layers)]\n",
        "\n",
        "  def create_positional_encoding(self, seq_length, d_model):\n",
        "    assert d_model % 2 == 0, \"Dimension model must be even\"\n",
        "\n",
        "    pos = np.arange(0, seq_length)[:, np.newaxis]  # (seq_length, 1)\n",
        "    pos_expanded = np.repeat(pos, d_model // 2, axis=1)  # (seq_length, d_model // 2)\n",
        "\n",
        "    power = np.arange(0, d_model, 2).astype(float) / d_model\n",
        "    div_term = np.power(10000, power)[np.newaxis, :]  # (1, d_model // 2)\n",
        "    div_term_expanded = np.repeat(div_term, seq_length, axis=0)  # (seq_length, d_model // 2)\n",
        "\n",
        "    pe = np.zeros((seq_length, d_model))  # (seq_length, d_model)\n",
        "    pe[:, 0::2] = np.sin(pos_expanded / div_term_expanded)  # (seq_length, d_model // 2)\n",
        "    pe[:, 1::2] = np.cos(pos_expanded / div_term_expanded)  # (seq_length, d_model // 2)\n",
        "\n",
        "    return pe\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    input: (batch_size, soruce_length, d_model)\n",
        "    output: (batch_size, soruce_length, d_model)\n",
        "    '''\n",
        "    x = self.embedding_layer.forward(x) # (N, L, d_model)\n",
        "    x = x + self.pos_encoding\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.encoder_layers[i].forward(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2VrIQbW1J3O",
        "outputId": "217dc573-411d-4aa3-b5e2-69912a1ca835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 10, 256)\n"
          ]
        }
      ],
      "source": [
        "# test encoder\n",
        "encoder = Encoder(source_length, d_model, num_heads, forward_dim, num_layers, vocab_size)\n",
        "x = np.random.randint(0, vocab_size, (batch_size, source_length)).astype(np.int64)\n",
        "output = encoder.forward(x)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZLGFW6xzLHa"
      },
      "source": [
        "## Deocder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "l18OdM-XRSr4"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer:\n",
        "  def __init__(self, d_model, num_heads, source_length, target_length):\n",
        "    self.casual_mha = MultiHeadAttention(d_model, num_heads, source_length, target_length)\n",
        "    self.cross_mha = MultiHeadAttention(d_model, num_heads, source_length, target_length)\n",
        "    self.ff = FeedForward(d_model, forward_dim)\n",
        "    self.layer_norm = LayerNorm(d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    ''' Decoder Layer\n",
        "\n",
        "    Inputs:\n",
        "      x: decoder input (N, L, d_model)\n",
        "      encoder_output: (N, L, d_model)\n",
        "    '''\n",
        "    x_prime = x\n",
        "    x = self.casual_mha.forward(x, x, x, casual=True) # (N, L, d_model)\n",
        "    x = self.layer_norm.forward(x + x_prime) # (N, L, d_model)\n",
        "\n",
        "    x_prime = x\n",
        "    x = self.cross_mha.forward(x, encoder_output, encoder_output) # (N, L, d_model)\n",
        "    x = self.layer_norm.forward(x + x_prime) # (N, L, d_model)\n",
        "\n",
        "    x_prime = x\n",
        "    x = self.ff.forward(x) # (N, L, d_model)\n",
        "    x = self.layer_norm.forward(x + x_prime) # (N, L, d_model)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcNZI4zQ3iMf",
        "outputId": "c1d45f34-e481-410f-c52f-59a27a610ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 10, 256)\n"
          ]
        }
      ],
      "source": [
        "# test decoder layer\n",
        "x = np.random.randn(batch_size, target_length, d_model)\n",
        "encoder_output = np.random.randn(batch_size, source_length, d_model)\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, source_length, target_length)\n",
        "output = decoder_layer.forward(x, encoder_output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "W96zJuO_RUYz"
      },
      "outputs": [],
      "source": [
        "class Decoder:\n",
        "  def __init__(self, vocab_size, d_model, target_length, num_layers, forward_dim):\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding_layer = Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = self.create_positional_encoding(target_length, d_model)\n",
        "    self.decoder_layers = [DecoderLayer(d_model, num_heads, source_length, target_length) for _ in range(num_layers)]\n",
        "    self.last_layer = Linear(d_model, vocab_size)\n",
        "\n",
        "  def create_positional_encoding(self, seq_length, d_model):\n",
        "    assert d_model % 2 == 0, \"Dimension model must be even\"\n",
        "\n",
        "    pos = np.arange(0, seq_length)[:, np.newaxis]  # (seq_length, 1)\n",
        "    pos_expanded = np.repeat(pos, d_model // 2, axis=1)  # (seq_length, d_model // 2)\n",
        "\n",
        "    power = np.arange(0, d_model, 2).astype(float) / d_model\n",
        "    div_term = np.power(10000, power)[np.newaxis, :]  # (1, d_model // 2)\n",
        "    div_term_expanded = np.repeat(div_term, seq_length, axis=0)  # (seq_length, d_model // 2)\n",
        "\n",
        "    pe = np.zeros((seq_length, d_model))  # (seq_length, d_model)\n",
        "    pe[:, 0::2] = np.sin(pos_expanded / div_term_expanded)  # (seq_length, d_model // 2)\n",
        "    pe[:, 1::2] = np.cos(pos_expanded / div_term_expanded)  # (seq_length, d_model // 2)\n",
        "\n",
        "    return pe\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    x = self.embedding_layer.forward(x) # (N, L, d_model)\n",
        "    x = x + self.pos_encoding\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.decoder_layers[i].forward(x, encoder_output)\n",
        "\n",
        "    x = self.last_layer.forward(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F94jsazK4dlV",
        "outputId": "6d96fdf6-3353-4de3-86bb-f99e217aac92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 10, 5000)\n"
          ]
        }
      ],
      "source": [
        "# test decoder\n",
        "x = np.random.randint(0, vocab_size, (batch_size, source_length)).astype(np.int64)\n",
        "encoder_output = np.random.randn(batch_size, source_length, d_model)\n",
        "decoder = Decoder(vocab_size, d_model, target_length, num_layers, forward_dim)\n",
        "output = decoder.forward(x, encoder_output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZhEx7JGzMi-"
      },
      "source": [
        "## TransFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "x_n-ll8oRWEp"
      },
      "outputs": [],
      "source": [
        "class TransFormer:\n",
        "  def __init__(self, source_length, target_length, forward_dim, num_layers, vocab_size, d_model):\n",
        "    self.encoder = Encoder(source_length, d_model, num_heads, forward_dim, num_layers, vocab_size)\n",
        "    self.decoder = Decoder(vocab_size, d_model, target_length, num_layers, forward_dim)\n",
        "\n",
        "  def forward(self, encoder_input, decoder_input):\n",
        "    encoder_output = self.encoder.forward(encoder_input)\n",
        "    decoder_output = self.decoder.forward(decoder_input, encoder_output)\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmWBhl_qRXb_",
        "outputId": "f89eade4-ad9b-4536-d429-881d90858b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mAssertion passed: The output shape matches the expected shape (batch_size, target_length, vocab_size).\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "transformer = TransFormer(source_length, target_length, forward_dim, num_layers, vocab_size, d_model)\n",
        "encoder_input = np.random.randint(0, vocab_size, (batch_size, source_length)).astype(np.int64)\n",
        "deocder_input = np.random.randint(0, vocab_size, (batch_size, target_length)).astype(np.int64)\n",
        "output = transformer.forward(encoder_input, deocder_input)\n",
        "assert output.shape == (batch_size, target_length, vocab_size)\n",
        "print(Fore.GREEN + \"Assertion passed: The output shape matches the expected shape (batch_size, target_length, vocab_size).\" + Style.RESET_ALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "_15TRGJD5laE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gKRXbo4lyGpM",
        "fjtnjoHe6dDt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
